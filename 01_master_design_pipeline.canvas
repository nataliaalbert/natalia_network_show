{
	"nodes":[
		{"id":"s3","type":"text","text":"## ğŸ”´ Step 3 â€” Review Existing Extraction Flowchart\n\n**Goal:** Critically examine each step of the 12-step extraction pipeline, answer 4 key questions per step, and identify design gaps before variation testing.\n\n**Input:** Miro flowchart (now in Obsidian), existing python scripts 04â€“05, Four Pillars Excel dictionary.\n\n**Method:** For each pipeline step ask:\n- What is the input? \n- What method is used? \n- What could go wrong? \n- What variable can I change? \n- Document answers in Obsidian canvas.\n\n**What could go wrong?** Discovering design flaws too late; inconsistent logic across steps; not linking pipeline steps to research questions\n\n**Variable to change:** Depth of review (surface audit vs full reconstruction); which steps get varied in Step 11","x":-200,"y":1200,"width":560,"height":520,"color":"#E07878"},
		{"id":"s2","type":"text","text":"## ğŸ”´ Step 2 â€” Confirm Policy Corpus Is Solid\n\n**Goal:** Validate that the corpus (PDFs + metadata) is clean, complete, and fit for analysis before any variation testing begins\n\n**Input:** ~7,000 NZ PDFs from Policy Commons, metadata CSV, output from scripts 01â€“03\n\n**Method:** Run data integration, NZ filter, and discrepancy checks (scripts 01, 02, 03). Manual spot-check of sample documents. Confirm file counts match metadata.\n\n**What could go wrong?** Silent data loss, corrupt PDFs, metadata mismatches, non-NZ docs slipping through the filter\n\n**Variable to change:** Filter criteria for NZ inclusion, handling of missing files, minimum document length threshold","x":-200,"y":580,"width":560,"height":410,"color":"#E07878"},
		{"id":"s1","type":"text","text":"## ğŸ”´ Step 1 â€” Learn Testing Methodology Design\n\n**Goal:** Understand how to design a rigorous testing and validation pipeline for computational text analysis before building anything\n\n**Input:** Literature on pipeline design, supervisor guidance (Markus), examples from network science and NLP benchmarking studies\n\n**Method:** Reading, discussion with Markus, mapping assumptions, understanding what varies and what you are testing for\n\n**What could go wrong?** Jumping to implementation before understanding design principles, building a fragile pipeline you can't defend methodologically\n\n**Variable to change:** Level of formalism in the testing approach (exploratory vs confirmatory)","x":-200,"y":0,"width":560,"height":440,"color":"#E07878"},
		{"id":"title","type":"text","text":"### ğŸ—ºï¸ Master Design Pipeline\n*Designed with Markus Â· Updated Feb 2026*\n\n**Colour key:**  ğŸ”´ Foundation (1â€“3)  Â·  ğŸŸ  Design Inputs (4â€“5)  Â·  ğŸŸ¡ Execute (6)  Â·  ğŸŸ¢ Analyse (6b)  Â·  ğŸ©µ Evaluate (7â€“9)  Â·  ğŸ”µ Plan & Prioritise (10â€“11)  Â·  ğŸŸ£ Prioritise & Run (12â€“13)  Â·  ğŸ”´ *Red arrows = recall < 90% â†’ loop back*","x":-200,"y":-200,"width":5600,"height":150,"color":"#5A5A5A"},
		{"id":"s5","type":"text","text":"## ğŸŸ  Step 5 â€” Design Dictionary Variations (4 Versions)\n\n**Goal:** Test how sensitive your network results are to different term lists by running 4 dictionary variants\n\n**Input:** Four Pillars Excel dictionary (human-built), NLP tools, corpus text, Wang et al. (2025) benchmarking methodology\n\n**Method:** Create 4 dictionary versions:\n- **Dict 1 â€” Human-built:** Your existing Four Pillars dictionary (expert-designed, NZ housing policy focus) âœ…\n- **Dict 2 â€” LLM-generated:** Prompt Claude/GPT to generate a policy term dictionary for NZ housing and social cohesion â€” zero-shot or few-shot following Wang et al. approach\n- **Dict 3 â€” Corpus-derived:** Extract high-frequency, high-TF-IDF terms directly from the corpus (data-driven, no prior assumptions)\n- **Dict 4 â€” Deliberately imbalanced:** One category over-represented, following Wang et al. multi-label benchmarking design â€” tests how imbalance affects recall and F1\n\n**What could go wrong?** LLM dict may overgeneralise (Wang et al. hallucination finding). Corpus-derived dict may miss rare but important terms. Imbalanced dict needs careful design to be a fair stress test.\n\n**Variable to change:** Prompt design for Dict 2, TF-IDF threshold for Dict 3, which category is over-represented in Dict 4, synonym expansion rules","x":760,"y":990,"width":560,"height":720,"color":"#F0A868"},
		{"id":"s4","type":"text","text":"## ğŸŸ  Step 4 â€” Design Dataset Variations (3 Samples)\n\n**Goal:** Test pipeline robustness across different data conditions by running it on three distinct corpora\n\n**Input:** Confirmed corpus from Step 2, research design decisions\n\n**Method:** Create 3 sub-corpora with different characteristics:\n- **Sample A:** Housing policy corpus from another country (international comparator â€” tests generalisability)\n- **Sample B:** Random mixed policy corpus + 3 NZ health policy documents (tests domain noise and non-housing content)\n- **Sample C:** Micro-sample of the existing 7,000-PDF NZ corpus (tests pipeline at scale with home corpus)\n\n**What could go wrong?** Samples too similar (low variance in results), or too small to be meaningful. Selection bias in Sample A if country chosen is too similar to NZ.\n\n**Variable to change:** Country for Sample A, size of micro-sample in Sample C, what counts as 'random' in Sample B","x":480,"y":120,"width":560,"height":570,"color":"#F0A868"},
		{"id":"s6","type":"text","text":"## ğŸŸ¡ Step 6 â€” Method Variation and Topic Identification\n\n**Goal:** Run the extraction pipeline using each combination of dataset and dictionary to produce 12 outputs (3 datasets Ã— 4 dictionaries). Also test alternative topic identification methods beyond term counting.\n\n**Input:** 3 dataset samples (Step 4) + 4 dictionary versions (Step 5) + extraction pipeline (scripts 04â€“05)\n\n**Method:** For each dataset Ã— dictionary combination, run the full pipeline and produce:\n- Term frequency counts per document\n- TF-IDF weighted feature tables\n- Topic identification using: (a) dictionary matching, (b) TF-IDF, (c) LLM-based sentence classification (following Wang et al.)\n\n**What could go wrong?** Combinatorial explosion â€” 12 outputs is manageable but documentation burden is high. Different methods may produce incompatible output formats.\n\n**Variable to change:** Topic identification method (dictionary / TF-IDF / LLM), granularity (document-level vs sentence-level), whether multi-label classification is applied","x":1360,"y":120,"width":560,"height":600,"color":"#F0D068"},
		{"id":"s6b","type":"text","text":"## ğŸŸ¢ Step 6b â€” Analyse the 12 Outputs\n\n**Goal:** Systematically compare results across all dataset Ã— dictionary combinations before moving to formal evaluation â€” look for patterns, anomalies, and surprises\n\n**Input:** 12 output files from Step 6 (term counts, feature tables, topic classifications)\n\n**Method:**\n- Compare term overlap across dictionaries within the same dataset\n- Compare topic distributions across datasets using the same dictionary\n- Flag cases where results diverge strongly â€” these are your most informative comparisons\n- Visualise using heatmaps or network diagrams\n\n**What could go wrong?** Confusing variation caused by method differences with variation caused by data differences â€” keep these dimensions separate in your analysis\n\n**Variable to change:** Comparison metric (overlap %, cosine similarity, rank correlation), visualisation format","x":1920,"y":990,"width":560,"height":610,"color":"#88C878"},
		{"id":"s7","type":"text","text":"## ğŸ©µ Step 7 â€” Design the Evaluation Framework\n\n**Goal:** Define how you will formally evaluate the 12 outputs â€” what counts as good, what counts as failure, and how you will compare across combinations\n\n**Input:** 12 outputs from Steps 6/6b, Wang et al. evaluation framework, Markus's guidance on pipeline design documents\n\n**Method:** Design evaluation framework covering:\n- Which metrics to use (see Step 9)\n- What you are evaluating against (ground truth â€” see Step 8)\n- How to handle multi-label outputs\n- Whether to use macro or micro averaging (Wang et al. recommend micro for imbalanced datasets)\n- How to document results systematically\n\n**What could go wrong?** Designing evaluation criteria after seeing results â€” build this before running full pipeline. Choosing metrics that don't match your research questions.\n\n**Variable to change:** Macro vs micro averaging, threshold for binary classification, whether to weight policy domains equally","x":2200,"y":0,"width":560,"height":340,"color":"#68C8B8"},
		{"id":"s9","type":"text","text":"## ğŸ©µ Step 9 â€” Choose Evaluation Metrics\n\n**Goal:** Select metrics that let you compare pipeline outputs against ground truth â€” borrowed from Wang et al. (2025)\n\n**Input:** Ground truth from Step 8, Wang et al. evaluation framework, your research questions\n\n**Method:** Apply four standard metrics (all defined in Wang et al.):\n\n- **Accuracy** = (TP + TN) / (FP + FN + TP + TN) â€” overall correctness\n- **Precision** = TP / (FP + TP) â€” how often positive predictions are right (penalises false alarms)\n- **Recall** = TP / (FN + TP) â€” how often true positives are caught (penalises misses) â€” *your pipeline currently targets â‰¥90% recall*\n- **F1-score** = harmonic mean of Precision and Recall â€” balances both\n\nUse **micro-averaging** across policy domains (following Wang et al.) because your dataset is likely imbalanced.\n\n**What could go wrong?** High accuracy masking poor recall on rare categories. F1 alone not enough if you care more about not missing policy terms than avoiding false positives.\n\n**Variable to change:** Whether to use micro vs macro averaging, whether to report per-category metrics, recall threshold (currently 90%)","x":2600,"y":1295,"width":560,"height":540,"color":"#68C8B8"},
		{"id":"s8","type":"text","text":"## ğŸ©µ Step 8 â€” Define Ground Truth and Benchmark\n\n**Goal:** Establish what 'correct' looks like for your pipeline â€” without this, your evaluation metrics are meaningless\n\n**Input:** Small manually-coded sample of documents, Wang et al. approach to ground truth construction, your NVivo codebook\n\n**Method:** Define ground truth through:\n- **Benchmark corpus:** Manually code a small sample (e.g. 50â€“100 documents or 500â€“1,000 sentences) using your NVivo codebook as reference\n- **Expert benchmark:** Your own coding + spot-check with Alex or Markus\n- **Reference comparison:** Use Wang et al. finding that fine-tuned BERT achieves 92.5% as an external benchmark for NLP methods\n- Document what you found and what surprised you â€” this becomes part of your methodology chapter\n\n**What could go wrong?** Ground truth too small to be statistically meaningful. Your own coding introduces bias. No inter-rater reliability check.\n\n**Variable to change:** Sample size for manual coding, whether to involve a second coder, which documents are included in ground truth set","x":2680,"y":650,"width":560,"height":340,"color":"#68C8B8"},
		{"id":"s11","type":"text","text":"## ğŸ”µ Step 11 â€” Create the Variable Matrix\n\n**Goal:** Build the pipeline design document Markus referred to â€” a structured table where rows = pipeline steps and columns = what you are varying\n\n**Input:** All decisions from Steps 3â€“10, pipeline review from Step 3\n\n**Method:** Create a matrix (recommend building in Excel or Obsidian table) structured as:\n\n**Rows** = each step in the 12-step extraction pipeline\n**Columns** = variables being tested:\n- Dataset variation (A / B / C)\n- Dictionary variation (Dict 1â€“4)\n- Method variation (dictionary matching / TF-IDF / LLM)\n- Unit of analysis (document / sentence)\n- Normalisation rules (strict / loose)\n\nEach cell = what that combination does and what you expect\n\nThis is your **core methodological transparency document** â€” it shows Markus and examiners that you have systematically explored the design space rather than just running one pipeline.\n\n**What could go wrong?** Matrix becomes too large to be useful. Focus on the variables that actually affect your research questions.\n\n**Variable to change:** Level of granularity in the matrix, which pipeline steps have the most cells filled in","x":3600,"y":1330,"width":560,"height":380,"color":"#7898D8"},
		{"id":"s10","type":"text","text":"## ğŸ”µ Step 10 â€” Build the Combination Test Plan\n\n**Goal:** Document exactly which dataset Ã— dictionary combinations you will run, in what order, and what you expect to find\n\n**Input:** 3 datasets (Step 4), 4 dictionaries (Step 5), evaluation framework (Step 7)\n\n**Method:** Create a test plan table with all combinations:\n\n| Run | Dataset | Dictionary | Expected challenge |\n|-----|---------|------------|-------------------|\n| 1 | Baseline NZ sample | Human-built | Baseline |\n| 2 | Baseline NZ sample | LLM-generated | Hallucination risk |\n| 3 | Baseline NZ sample | Corpus-derived | Domain specificity |\n| 4 | Baseline NZ sample | Imbalanced | Recall degradation |\n| 5â€“8 | International housing | All 4 dicts | Generalisability |\n| 9â€“12 | Mixed/health corpus | All 4 dicts | Domain noise |\n\nPriority order: Run 1 â†’ 2 â†’ 3 first (baseline corpus, three main dictionaries) before moving to other datasets\n\n**What could go wrong?** Running combinations in random order makes it hard to isolate what changed. No pre-registered expectations = confirmation bias risk.\n\n**Variable to change:** Which combinations to prioritise, whether to run all 12 or a subset first","x":3480,"y":170,"width":560,"height":760,"color":"#7898D8"},
		{"id":"s13","type":"text","text":"## ğŸŸ£ Step 13 â€” Run Code, Evaluate Output, and Document\n\n**Goal:** Execute the pipeline for each prioritised combination, apply evaluation metrics, and produce documented results ready for thesis writing\n\n**Input:** Scripts 01â€“05 (and future scripts), variable matrix, evaluation framework, ground truth\n\n**Method:**\n- Run each combination in priority order from Step 12\n- Record metrics (Accuracy, Precision, Recall, F1) for each run\n- Compare against ground truth and against other runs\n- Write a brief memo in Obsidian after each run: what worked, what surprised you, what to adjust\n- Update variable matrix with actual results\n- If recall < 90% â†’ return to Steps 5 or 6 and adjust\n- If results are stable across combinations â†’ you have a defensible pipeline\n\n**What could go wrong?** Not documenting as you go â€” you WILL forget what you changed between runs. Treating a failed run as wasted time rather than methodological information.\n\n**Variable to change:** Everything â€” this is where the matrix comes alive. Document each change.\n\n**Output:** Results table + documented pipeline + thesis methodology section draft","x":4840,"y":1075,"width":560,"height":760,"color":"#A888D8"},
		{"id":"s12","type":"text","text":"## ğŸŸ£ Step 12 â€” Prioritise Combinations to Run First\n\n**Goal:** Be strategic about which combinations give you the most methodological insight for the least effort\n\n**Input:** Variable matrix (Step 11), test plan (Step 10), time and compute constraints\n\n**Method:** Priority order for first runs:\n\n1. ğŸ¥‡ **Baseline corpus + Human-built dictionary** â€” your reference point, everything else is compared to this\n2. ğŸ¥ˆ **Baseline corpus + LLM-generated dictionary** â€” tests whether LLM dict matches human judgement (Wang et al. question)\n3. ğŸ¥‰ **Baseline corpus + Corpus-derived dictionary** â€” tests whether the data itself surfaces the right terms without prior knowledge\n4. Then: Baseline corpus + Imbalanced dictionary â€” stress test\n5. Then: International corpus Ã— all dictionaries â€” generalisability test\n6. Finally: Mixed/health corpus Ã— all dictionaries â€” domain noise test\n\n**What could go wrong?** Getting stuck perfecting Run 1 before moving on. Set a time limit per run before moving to the next.\n\n**Variable to change:** Whether you run all 12 for your thesis or select a theoretically motivated subset to report","x":4280,"y":175,"width":560,"height":645,"color":"#A888D8"}
	],
	"edges":[
		{"id":"e1","fromNode":"s1","fromSide":"bottom","toNode":"s2","toSide":"top","color":"#E07878","label":"understand design first"},
		{"id":"e2","fromNode":"s2","fromSide":"bottom","toNode":"s3","toSide":"top","color":"#E07878","label":"corpus confirmed"},
		{"id":"e3","fromNode":"s3","fromSide":"right","toNode":"s4","toSide":"left","color":"#F0A868","label":"pipeline reviewed"},
		{"id":"e4","fromNode":"s3","fromSide":"right","toNode":"s5","toSide":"left","color":"#F0A868","label":"pipeline reviewed"},
		{"id":"e5","fromNode":"s4","fromSide":"bottom","toNode":"s5","toSide":"top","color":"#F0A868","label":"3 datasets ready"},
		{"id":"e6","fromNode":"s4","fromSide":"right","toNode":"s6","toSide":"left","color":"#F0D068","label":"3 datasets ready"},
		{"id":"e7","fromNode":"s5","fromSide":"right","toNode":"s6","toSide":"left","color":"#F0D068","label":"4 dictionaries ready"},
		{"id":"e8","fromNode":"s6","fromSide":"right","toNode":"s6b","toSide":"left","color":"#88C878","label":"12 outputs produced"},
		{"id":"e9","fromNode":"s6b","fromSide":"top","toNode":"s7","toSide":"left","color":"#68C8B8","label":"patterns identified"},
		{"id":"e10","fromNode":"s7","fromSide":"bottom","toNode":"s8","toSide":"top","color":"#68C8B8","label":"framework designed"},
		{"id":"e11","fromNode":"s8","fromSide":"bottom","toNode":"s9","toSide":"top","color":"#68C8B8","label":"ground truth defined"},
		{"id":"e12","fromNode":"s9","fromSide":"right","toNode":"s10","toSide":"left","color":"#7898D8","label":"metrics chosen"},
		{"id":"e13","fromNode":"s10","fromSide":"bottom","toNode":"s11","toSide":"top","color":"#7898D8","label":"combinations mapped"},
		{"id":"e14","fromNode":"s11","fromSide":"right","toNode":"s12","toSide":"left","color":"#A888D8","label":"matrix built"},
		{"id":"e15","fromNode":"s12","fromSide":"right","toNode":"s13","toSide":"left","color":"#A888D8","label":"priority order set"}
	]
}